#####How to evaluate the regression line, a cost for given a linear regression line?
* Common cost associated with a specific fit to the data is called the residual sum of squares. 
* The residual is the difference of your prediction and your actual observation.
* Sum over all the difference and try to find the best line, which minimize the residual sum of squares.
* Resulting W and W hat, the set of W hat zero, and W hat one, intercept and slope

#####Linear relationship
* Quadratic fit still called linear regression
* 13th order polynomial, overfitting
* Simulating predictions, training set(use as a proxy for predictions) and testing set(hold out)
* Only work well if ther is enough observations
* Training error: sum of residual squares in the training set
* Test error: for hold out test 

#####Training/Test curves
* Have trainging error decreases with increasing model order
* Have test error decreases at first and then increase again in the curve

#####Adding features
* Fitting a hyper plane

#####Regression applications
* Predict salary after taking machine learning specialization
* Predict stock price
* Predict sitting on Twitter
* Predict the temprature for a smart house

#####Regression ML block diagram
* Feature extractor
* Some set of parameters have the weights on features, coefficients
* Error metric, residual sum of sqaures
* Training data set, feature extraction process, machine learning model, prediction, quality measure
* 
